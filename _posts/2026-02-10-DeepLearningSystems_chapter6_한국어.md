---
title: 딥러닝 시스템 6장
author: khw
date: 2026-02-10
categories: [Study, DeepLearingSystems]
tags: [LAB]
pin: true
math: true
mermaid: true
---

## 모델 크기 줄이기

컴퓨터는 실제 수치를 보통 8, 16, 32 또는 64비트의 이진수(비트) 집합으로 표현합니다. 더 많은 비트를 사용할수록 수치의 범위가 넓어지고 정밀도, 즉 수치 표현의 정확도가 높아집니다. 모델의 수치 형식(numerical format)은 계산 및 통계적 성능에 영향을 미칠 수 있습니다. 더 작은 수치 표현을 사용하면 사이클당 연산 횟수를 늘리고 메모리, 메모리 대역폭, 네트워크 대역폭 및 전력 소비를 줄일 수 있습니다. 특히 워크로드가 메모리 대역폭 제한(메모리 대역폭에 의해 병목이 발생하는 경우)인 경우, 수치 표현을 줄이면 이러한 병목 현상이 완화되어 계산 성능이 향상됩니다. 연산 제한(가용한 연산 능력에 의해 병목이 발생하는 경우)인 경우, 하드웨어 설계자는 주어진 다이(die) 면적에 더 작은 수치 형식의 곱셈기를 더 많이 배치하여 계산 성능을 향상시킬 수 있습니다. 그러나 더 작은 수치 표현을 사용하면 일부 모델에서는 통계적 성능이 저하될 수 있습니다.

그림 1.17은 각각의 부호(sign), 지수(exponent), 가수(mantissa) 비트 수를 가진 다양한 수치 형식을 보여줍니다. 지수 비트는 범위를 결정하고, 가수 비트는 정밀도를 결정합니다. 예를 들어, $fp32$와 $bf16$은 동일한 범위 계수를 갖지만, $fp32$가 더 높은 정밀도를 제공합니다.

모델 크기를 줄이는 데 사용되는 네 가지 주요 기술은 다음과 같습니다.

1. 수치 표현 줄이기 (양자화);
2. 모델의 일부를 가지치기(pruning)하고 가지치기된 모델을 압축하기;
3. 지식을 더 작은 모델로 증류(distilling)하기;
4. 작은 모델에 보상을 주는 NAS(신경망 아키텍처 검색) 사용하기.

대부분의 상용 애플리케이션은 학습 및 추론 워크로드에 $fp32$를 사용하지만, 더 낮은 수치 형식이 빠르게 채택되고 있습니다. 구체적으로 학습 및 추론에는 반정밀도 부동 소수점($fp16$) 및 bfloat16($bf16$)이, 일부 워크로드의 추론에는 $int8$이 사용되며, 모두 MAC 연산을 위해 32비트 누산(accumulation)을 사용합니다. $fp32$ 누산기와 함께 $bf16$ 또는 $fp16$ 곱셈기를 사용하면 학습 및 추론의 정확도 손실이 거의 없거나 무시할 수 있는 수준입니다. $int32$ 누산기와 함께 $int8$ 곱셈기를 사용하면 일부 추론 워크로드에서 약간의 또는 최소한의 정확도 손실이 발생합니다. 활성화(activations)를 16비트 형식으로 저장하면 하드웨어가 16비트 곱셈을 지원하지 않더라도 메모리 및 대역폭 소비를 거의 2배 줄일 수 있다는 점에 유의하십시오 [Dev17].

학습은 추론보다 더 큰 수치 표현을 필요로 하며, 특히 기울기(gradient)와 가중치 업데이트의 동적 범위를 포착하기 위해 그렇습니다. 그림 6.1은 두 개의 별도 학습 에포크(epoch)에 걸쳐 ResNet-110 텐서의 로그 2 밑 절대값 히스토그램을 보여주며, 가중치 업데이트 값의 범위가 더 크다는 것을 보여줍니다.

![Image](/assets/img/posts/2026-02-10-DeepLearningSystems_chapter6_artifacts/figure6.1.png)

그림 6.1: CIFAR 데이터셋을 사용한 두 개의 별도 학습 에포크에서의 ResNet-110 가중치, 활성화 및 가중치 업데이트 분포. [KWW+17]에서 저자의 허가를 받아 수정됨.

활발한 연구 분야 중 하나는 8비트 및 4비트로 값을 더 잘 표현하면서 실리콘으로 구현하기 간단한 수치 표현을 개발하는 것입니다. 더 작은 수치 표현을 사용하면 하드웨어가 해당 표현에서 사이클당 더 높은 피크 연산을 지원하지 않더라도 학습 및 추론을 개선할 수 있는데, 이는 메모리 대역폭 절감이 흔히 발생하는 메모리 대역폭 제한 계층을 가속화하기 때문입니다.

모델은 일반적으로 과매개변수화(overparameterized)되어 있어 학습을 용이하게 하고 학습 후 모델 크기를 줄일 수 있는 기회를 제공합니다. 학습된 모델에는 보통 여러 개의 작은 가중치가 있습니다. 이를 0으로 강제하면 통계적 영향은 거의 없이 계산상의 이점을 얻을 수 있습니다. 이 과정을 가지치기(pruning)라고 하며 결과적으로 희소(sparse) 모델이 생성됩니다. 6.3절에서 논의되는 모델 희소성에는 구조적(structured) 희소성과 비구조적(unstructured) 희소성의 두 가지 유형이 있습니다.

희소 모델의 주요 이점은 압축률 향상입니다. 압축은 압축 해제를 위한 약간의 추가 연산을 희생하여 메모리 사용량과 메모리 대역폭 소비를 줄입니다. 이 추가 압축 해제 시간은 일반적으로 압축되지 않은 데이터를 전송하는 데 걸리는 추가 시간보다 적기 때문에 압축이 유리합니다.

작은 모델을 훈련시켜 큰 훈련된 모델의 출력을 생성하게 할 수 있습니다. 더 큰 훈련된 모델(교사 모델)의 지식이 더 작은 모델(학생 모델)로 증류됩니다. 이 방법을 지식 증류(knowledge distillation)라고 합니다.

6.1절에서는 생산 환경에서 채택된 다양한 16비트 및 8비트 수치 형식과 기타 유망한 형식을 검토합니다. 6.2절에서는 모델을 $fp32$에서 $int8$로 양자화하는 기술을 논의합니다. 6.3절에서는 가지치기 및 압축 기술을 검토합니다. 6.4절에서는 지식 증류에 대해 더 자세히 설명합니다.

## 6.1 수치 형식 (NUMERICAL FORMATS)

가장 인기 있고 널리 채택된 형식은 학습과 추론 모두에 사용되는 $fp32$입니다. 업계는 학습 및 추론을 위해 $fp16$ 및 $bf16$으로, 그리고 일부 워크로드의 추론을 위해 $int8$로 이동하고 있습니다. Nvidia는 행렬 곱셈을 위한 비표준 $fp19$ 형식(가끔 bfloat19라고도 함)을 도입했는데, 이는 $bf16$의 범위와 $fp16$의 정밀도를 결합한 것입니다. Intel과 IBM은 비표준 $fp8$ 형식을 탐구했습니다. 그림 1.17은 각각의 부호, 지수, 가수 비트 수를 가진 다양한 수치 형식을 보여줍니다. 가수는 유효숫자(significand)라고도 하며, 로그 문헌에서 로그의 소수 부분을 지칭하는 데 사용되는 가수(mantissa)라는 용어와 혼동해서는 안 됩니다.

다양한 하드웨어 사용을 내다볼 때, 다양한 개발 단계에서 사용되고 있거나 사용될 수 있는 수치 형식은 다음과 같습니다.

- 토폴로지 연구 및 토폴로지 설계: $fp32$;
- 데이터 센터에서의 생산 모델 학습: $fp32$, $bf16$, $fp16$, 및 $fp19$; 제한적인 $fp8$;
- 데이터 센터에서의 생산 모델 서빙(serving): $fp16$, $bf16$, 및 $fp8$; 일부 $int8$; 극히 제한적인 $int4$;
- 엣지 디바이스에서의 생산 모델 서빙: $fp16$ (전력 제약에 따라 다름), $int8$, 및 $fp8$; 일부 $int4$.

TensorFlow, PyTorch, MXNet, OpenVINO, TensorRT와 같은 딥러닝 라이브러리는 $int8$, $fp16$, $bf16$, $fp32$를 지원합니다. 다른 형식이 채택되려면 하드웨어 및 프레임워크 지원이 필요합니다.

표 6.1은 부동 소수점 숫자의 범위, 최소 및 최대 양수 값, 그리고 다양한 수치 형식에 걸친 최대 수치 오류를 보여줍니다. $fp8\text{-}ibm$은 IBM이 도입하고 아래에서 논의되는 8비트 부동 소수점 형식을 의미합니다. u f 4; 8 g는 f 4; 8 g 비트 부호 없는 정수를, s f 4; 8; 16; 32 g는 f 4; 8; 16; 32 g 비트 부호 있는 정수를 나타내며, ( n S ; n E ; n M )은 부동 소수점 형식의 부호, 지수 및 가수 비트 수를 각각 나타냅니다. 따라서 ( 1; 8; 23 )은 부호 비트 1개, 지수 비트 8개, 가수 비트 23개인 형식을 나타내며, 이는 $fp32$에 해당합니다. 지수 비트는 범위를 결정하고 가수 비트는 정밀도를 결정합니다. 주어진 부동 소수점 표현의 최대 수치 오류는 부동 소수점 숫자에 최대 오류(Maximum Error)를 곱한 것입니다.

표 6.1: 다양한 수치 형식 비교. 주어진 부동 소수점 표현의 최대 수치 오류는 부동 소수점 숫자에 최대 오류(Maximum Error)를 곱한 것입니다.

| 형식<br>$(n_S, n_E, n_M)$ | 양수<br>범위 | 양수<br>최소값 | 양수<br>최대값 | 최대<br>오류 |
|--------------------------|----------------|------------------|------------------|---------------|
| $fp32$ (1, 8, 23) | $[2^{-126}, 2^{128}]$ | $1.4 \times 10^{-45}$ | $3.40 \times 10^{38}$ | $6.0 \times 10^{-8}$ |
| $fp19$ (1, 8, 10) | $[2^{-126}, 2^{128}]$ | $1.1 \times 10^{-41}$ | $3.40 \times 10^{38}$ | $4.9 \times 10^{-4}$ |
| $bf16$ (1, 8, 7) | $[2^{-126}, 2^{128}]$ | $9.1 \times 10^{-41}$ | $3.39 \times 10^{38}$ | $3.9 \times 10^{-3}$ |
| $fp16$ (1, 5, 10) | $[2^{-14}, 2^{16}]$ | $6.0 \times 10^{-8}$ | $65504$ | $4.9 \times 10^{-4}$ |
| $fp8$ (1, 5, 2) | $[2^{-14}, 2^{16}]$ | $1.5 \times 10^{-5}$ | $57344$ | $0.125$ |
| $fp8$ (1, 4, 3) | $[2^{-6}, 2^{8}]$ | $2.0 \times 10^{-3}$ | $240$ | $0.0625$ |
| $fp8\text{-}ibm$ (1, 4, 3) | $[2^{-10}, 2^{4}]$ | $1.2 \times 10^{-4}$ | $15$ | $0.0625$ |
| $s32$ | $[1, 2^{31} - 1]$ | $1$ | $2.15 \times 10^{9}$ | $0.5$ |
| $s16$ | $[1, 2^{15} - 1]$ | $1$ | $32767$ | $0.5$ |
| $s8$ | $[1, 2^{7} - 1]$ | $1$ | $127$ | $0.5$ |
| $u8$ | $[1, 2^{8} - 1]$ | $1$ | $255$ | $0.5$ |
| $s4$ | $[1, 2^{3} - 1]$ | $1$ | $7$ | $0.5$ |
| $u4$ | $[1, 2^{4} - 1]$ | $1$ | $15$ | $0.5$ |

## 또는 정수 표현의 경우 0:5입니다.

16비트(구체적으로 $bf16$ 또는 $fp16$)로 모델을 학습하려면 일반적으로 다음이 필요합니다.

- 16비트 피연산자를 $fp32$로 누적하고, 누적된 합계를 합산한 후 다시 16비트로 변환하는 MAC 연산자 (하드웨어 로직은 비용 절감을 위해 (1; 8; 21)과 같이 더 적은 비트의 레지스터에 누적할 수 있음에 유의);
- $fp32$로 누적되고 결과가 16비트로 변환되는 리덕션(합계);
- $fp32$ 또는 16비트에서의 활성화 함수;
- 16비트로 저장된 활성화;
- 가중치 업데이트에 사용되는 $fp32$ 가중치 사본 (업데이트는 16비트 기울기를 사용);
- 다음 반복(iteration)을 위해 16비트로 변환된 업데이트된 가중치의 사본.

처음 세 가지 항목은 16비트 또는 8비트 형식을 사용하는 추론에도 적용됩니다. 두 경우 모두 수치 오버플로를 방지하기 위해 더 큰 수치 형식으로 누적하는 것이 권장됩니다 (표기법: MAC 소스 ! MAC 목적지): $\{fp16, bf16\} \rightarrow fp32$ 및 $int8 \rightarrow s32$ (부호 있는 $int32$).

부동 소수점 16비트 bfloat($bf16$)은 Google에서 브레인 부동 소수점(brain floating-point)으로 도입했습니다. 모델은 가산 잡음(additive noise)에 강건하며, 사실 4.1절에서 논의된 바와 같이 가중치 감쇠 정규화(weight decay regularization) 형태로 모델을 학습할 때 잡음을 추가하는 것이 일반적인 관행입니다. $fp32$의 23비트 가수 비트를 $bf16$의 7비트로 줄이는 것은 모델에 잡음을 주입하는 것으로 해석할 수 있습니다. $bf16$은 $fp32$와 동일한 범위 계수를 유지하며 특히 기울기의 범위를 지원하는 데 유용합니다. 실험에 따르면 $bf16$으로 학습된 모델은 하이퍼파라미터 변경이나 목적 함수 비용 스케일링 없이 동일한 반복 횟수로 $fp32$로 학습된 모델과 거의 동일한 정확도를 가집니다 [KMM+19]. 그러나 이러한 관찰이 유효하지 않은 아웃라이어 모델이 있을 수 있습니다. 또한 클래스 수가 $2^{nM}$ 또는 127보다 큰 경우 비용 함수에 $fp32$를 사용해야 합니다. 게다가 softmax만 $bf16$을 사용할 수 있지만, 다양한 구현에서는 softmax 함수와 비용 함수를 결합합니다. 이러한 구현은 $fp32$를 사용해야 합니다.

$bf16$은 주로 학습용으로 설계되었지만(기울기를 표현하기 위한 큰 지수), 추론에도 사용되어 $fp32$ 대비 유사한 계산 이득을 얻습니다. Google TPU v2-4, Habana Gaudi AI 프로세서, 3세대 Intel Xeon Scalable 프로세서(코드명 Cooper Lake), Arm 기반 Neoverse N2 'Zeus' CPU, Nvidia A100 GPU에는 $bf16$ 곱셈기가 있습니다.

부동 소수점 16비트 반정밀도($fp16$)는 추론 및 학습에 사용되며, 학습의 경우 종종 손실 스케일링(loss-scaling)이라는 기술이 필요합니다. 학습 중, 특히 초기 단계에서 많은 활성화 기울기의 크기가 $fp16$의 지원 범위 아래로 떨어져 0으로 잘리는 경우가 많으며 $fp16$의 상위 범위는 활용되지 않습니다. 손실(더 정확하게는 비용 또는 목적 함수)을 스케일링하면 매우 작은 값을 표현할 수 없는 문제를 완화하고 더 높은 범위를 사용할 수 있습니다. 구체적으로, 활성화 기울기가 $fp16$ 상위 범위를 넘지 않도록 하면서 비용을 1보다 훨씬 큰 값($\gg$ 1)으로 스케일링합니다. 그런 다음 가중치 업데이트 전에 가중치 기울기를 동일한 비율로 언스케일링(unscaling)합니다. 또한 0-255 RGB 입력 이미지 값을 0-1로 정규화하고 활성화에 배치 정규화를 추가하면 오버플로 위험이 줄어듭니다 [Wu19]. Nvidia GPU, AMD Radeon GPU, Huawei Atlas 및 Ascend 프로세서, Graphcore Colossus에는 $fp16$ 곱셈기가 있습니다.

$fp16$에 비해 $bf16$의 주요 장점은 경험적 튜닝이 필요한 손실 스케일링을 구현할 필요가 없다는 것입니다. 이 장점은 학습 전반에 걸쳐 기울기 분포의 변동이 커서 소프트웨어 복잡성을 증가시키는 GNMT 및 Transformer와 같이 동적 손실 스케일링(및 동적 튜닝)이 필요한 모델에서 특히 중요합니다 [MSD+19]. OpenSeq2Seq와 같은 일부 도구는 일부 모델에 대해 동적 손실 스케일링을 자동화할 수 있습니다 [KGG+18].

$fp16$에 비해 $bf16$의 단점은 가수 비트가 3개 더 적다는 것입니다. 그 비트들로부터 이점을 얻는 정밀도에 민감한 워크로드가 있을 수 있습니다. $bf16$의 상위 범위 값은 사용되지 않으므로 대부분의 학습 워크로드에 8비트 지수가 필요한지에 대한 의문이 제기됩니다. 예를 들어 Facebook은 DLRM 학습에서 임베딩 레이어를 저장하기 위해 ($bf16$이 아닌) $fp16$을 사용합니다(임베딩 레이어의 MAC 연산자는 $fp32$에서 발생) [ZYY18]. 학습 프로세서를 설계할 때, 한 가지 형식($fp16$ 또는 $bf16$)만 지원하는 기존 하드웨어에서의 전환을 용이하게 하기 위해 두 가지 모두 지원하는 것(19비트 (1, 8, 10) $fp19$ 부동 소수점 회로 유닛 사용)이 권장됩니다.

TensorFloat-32(19비트 부동 소수점, $tf32$)는 Ampere 아키텍처부터 Nvidia가 도입했습니다. TensorFloat-32는 $fp32$ 누산과 함께 $fp19$ MAC을 사용합니다. 행렬 곱셈에 사용되는 MAC 연산을 제외한 모든 연산과 저장은 $fp32$에서 발생합니다. 이러한 $fp32$ MAC은 $fp19$ MAC으로 대체되고 특수 텐서 코어로 가속화됩니다. 이 대체는 프레임워크 최종 사용자에게 숨겨질 수 있으며, 모든 것이 $fp32$에서 실행되는 것처럼 보입니다. $fp32$에서 $fp19$로의 변환(마지막 13개의 가수 비트 잘라냄)과 $fp19$ MAC은 CUDA 컴파일러에 의해 관리되며 cuDNN 및 cuBLAS와 같은 저수준 라이브러리에 의해 숨겨집니다. $fp19$ MAC의 정확도가 $fp32$ MAC과 동일하다고 보장되지는 않습니다. 그러나 $bf16$(이는 $fp19$로 이어짐)을 사용한 경험적 증거는 딥러닝 워크로드의 경우 정확도 차이가 미미함을 시사합니다. 하지만 알려지지 않은 아웃라이어가 존재할 수 있습니다 [KMM+19].

$tf32$의 주요 장점은 채택의 용이성입니다. DL 라이브러리 변경(활성화 플래그 제외)이 필요 없으며 즉시 작동합니다. 단점은 종종 더 큰 병목 현상이 되는 16비트 형식에 비해 메모리 또는 대역폭 절감 효과가 없다는 것입니다.

정수 16($int16$) 학습은 하이퍼파라미터 튜닝 없이 일부 모델에서 입증되었습니다 [KWW+17, DMM+18]. 텐서 내의 가중치, 활성화, 가중치 기울기 및 활성화 기울기의 분포는 $int16$과 전체 텐서에 대한 하나의 공유 스칼라를 사용하여 표현할 수 있습니다. 이 스칼라는 범위를 최대화하고 오버플로를 최소화하도록 동적으로 조정됩니다. 가중치 및 활성화 분포는 연속적인 학습 반복에서 급격하게 변하지 않습니다. 기울기 분포는 더 빠르게 변합니다. 프로그램은 분포를 모니터링하고 필요에 따라 각 텐서에 대한 지수를 조정할 수 있습니다.

학습의 경우, $int16$은 생산 환경에서 사용되지 않습니다. 특히 기울기 텐서에 대해 공유 지수를 관리하는 복잡성이 추가되므로 $int16$보다 $bf16$ 및 $fp16$이 선호됩니다. 추론의 경우 $int16$이 어느 정도 채택되었습니다. Habana Goya는 $int8$보다 더 정밀도가 필요한 워크로드에 $int16$을 사용합니다(Habana Goya는 다른 형식도 지원함) [Hab19].

정수 8($int8$)은 일부 추론 워크로드에서 빠르게 채택되고 있습니다. $int8$을 사용하면 32비트에서 8비트로 양자화할 때 정보 손실로 인해 통계적 성능이 저하되는 경우가 많습니다. 일부 애플리케이션의 경우 통계적 성능의 작은 하락도 금전적 부정적 영향을 미칠 수 있어 용납되지 않습니다. 특히 제품 추천 결과의 관련성이 떨어지면 구매가 줄어듭니다. 6.2절에서 논의된 통계적 손실을 줄이는 기술들이 있습니다. $int8$을 사용한 학습은 업계와 관련이 없는 몇 가지 간단한 모델에 대한 학술 연구로 제한된다는 점에 유의하십시오.

대부분의 $int8$ 양자화 기술에는 두 가지 주요 과제가 있습니다. 첫째, $int8$의 균일한 분포는 대부분의 정보가 존재하는 고밀도 영역에서 값을 더 잘 표현하기 위한 미세한 입도(granularity)를 허용하지 않습니다. 더 나은 접근 방식은 고밀도 영역에서는 높은 입도를, 저밀도 영역에서는 낮은 입도를 갖는 불균일한 수치 형식을 사용하는 것입니다. 이는 32비트에서 8비트로의 정보 손실을 줄입니다. $fp8$과 같은 일부 제안은 아래에서 논의됩니다.

둘째, 활성화의 양자화 계수를 미리 계산하는 것은 $int8$의 계산 이점을 극대화하는 데 필요하지만 개발자에게 추가적인 노력을 요구합니다. 생산 데이터로 활성화 값의 분포를 추정하려면 생산 데이터와 유사한 특성을 가진 데이터 샘플을 사용해야 합니다. 이를 위해서는 모델을 양자화하는 개발자가 생산 데이터와 유사한 샘플에 접근할 수 있어야 합니다.

이러한 과제에도 불구하고 $int8$은 추론용으로 판매되는 모든 주요 하드웨어에서 지원됩니다. Google은 일부 MLP, CNN 및 LSTM 기반 모델의 경우 TPU에서, RNN 모델을 사용한 음성 인식이 포함된 Google Pixel 폰에서 $int8$을 생산 환경에 사용합니다. Facebook(및 다른 많은 회사들)도 다양한 워크로드에서 $int8$을 사용합니다 [JYP+17, HSP+19, PNB+18]. Facebook은 또한 통계적 성능에 영향을 주지 않으면서 추천 서빙을 위해 임베딩 레이어에서 4비트로 양자화하는 것을 시연했습니다.

특히 $int8$ 추론은 다양한 CNN 모델에서 작동하는 것으로 나타났습니다 [GMY+19]. 그러나 MobileNet 및 ResNeXt와 같은 일부 CNN 모델과 BERT와 같은 다양한 비-CNN 모델은 양자화로 인한 정보 손실에 더 민감하며 허용 가능한 통계적 성능을 달성하기 위해 추가적인 노력이 필요합니다 [SDY+19]. 허용 가능한 성능 저하 수준은 다양하지만, 대부분의 회사에서 1% 이상의 저하는 허용되지 않고 0.5% 미만은 허용되며, 그 사이는 애플리케이션에 따라 다릅니다. 추천 시스템은 수익화 영향으로 인해 0.01% 수준의 더 엄격한 임계값을 가집니다.

부동 소수점 8비트($fp8$)는 Microsoft가 FPGA에서 2개 또는 3개의 가수 비트를 사용하여 사용합니다(Microsoft는 $fp9$도 사용함). $fp8$은 모바일 장치에서의 모델 학습을 시연하기 위해 심층 학습 신경 처리 장치(LNPU)와 같은 일부 ASIC에서 연구원들에 의해 구현됩니다(LNPU는 $fp8$ 및 $fp16$ 혼합 정밀도 학습을 사용함) [CFO+18, LLH+19]. Intel과 IBM은 다양한 워크로드에 대해 성능 손실이 미미한 수준으로 학습 및 추론에 $fp8$ 곱셈(각각 $fp32$ 및 $fp16$으로 누적됨)을 사용할 수 있음을 보여줍니다 [CBG+20, MSD+19, SCC+19].

표준화된 $fp8$ 형식은 없습니다. 가장 일반적인 형식은 $(1, 5, 2)$ 및 $(1, 4, 3)$입니다. $(1, 5, 2)$ 형식은 기울기의 동적 범위를 더 잘 나타냅니다. 8비트 형식으로 학습할 때의 특별한 과제는 RNN 및 정규화 레이어가 없는 모델에 있는데, 이들은 오류에 더 민감하기 때문입니다. RNN에서는 기울기 오류가 빠르게 증가할 수 있으며, 일반적인 정규화의 부족은 불규칙한 텐서 값 분포를 초래할 수 있습니다.

IBM은 순전파 및 역전파에 각각 하이브리드 $(1, 4, 3)$ 및 $(1, 5, 2)$ 접근 방식을 제안했습니다. 이는 손실 스케일링과 확률적 반올림(stochastic rounding)을 사용하고 입력 및 마지막 레이어를 $fp16$으로 유지합니다 [SCC+19]. $(1, 4, 3)$ 형식은 가중치 및 활성화의 분포와 더 잘 일치하도록 커버리지 범위를 $2^{-4}$만큼 이동시키기 위해 -4 고정 지수 바이어스를 사용하여 수정됩니다. 이 형식은 표 6.1에서 $fp8\text{-}ibm$으로 지칭됩니다. 이 형식에는 두 가지 주요 과제가 있습니다. 첫째, GNMT 및 Transformer와 같은 일부 모델은 적절히 수렴하기 위해 동적 손실이 필요하며, 이는 소프트웨어 복잡성을 증가시킵니다. 둘째, $fp16$에 비해 작은 값에 대한 표현이 제한적이라(가장 작은 양수는 $(1, 5, 2)$에서 $1.5\times10^{-5}$인 반면 $(1, 5, 10)$에서는 $6.0\times10^{-8}$), 종종 언더플로가 발생합니다.

Intel은 두 가지 방법을 제안했으며, 둘 다 $(1, 5, 2)$ 형식을 사용합니다. 한 가지 방법은 광범위한 값을 표현하기 위해 텐서당 이동 및 스케일(shifted and squeezed FP8 (S2FP8)) 매개변수를 사용합니다. S2FP8은 손실 스케일링, 확률적 반올림, 그리고 첫 번째 및 마지막 레이어에 대한 $fp32$의 필요성을 완화합니다. 주요 가중치 및 누적은 $fp32$로 이루어집니다 [CBG+20]. 그러나 S2FP8은 텐서 분포의 통계를 추적하고($int16$ 학습과 유사) 이동 및 스케일 매개변수를 업데이트해야 하므로 소프트웨어 복잡성이 증가합니다.

다른 방법은 값의 범위를 개선하고 $fp8$ 학습에서 관찰되는 일반적인 언더플로를 줄이기 위해 향상된 손실 스케일링을 사용합니다. 이 방법은 스케일링 팩터에 대해 동적으로 증가하는 최소 임계값을 사용하는 손실 스케일링을 이용합니다. 최소 임계값을 사용하면 더 높은 손실 스케일 값을 유지하기 위해 가짜 오버플로를 무시합니다. 그러나 이 방법은 이 임계값을 언제 조정할지 결정하기 위해 학습 비용을 관찰해야 합니다.

$int8$ 추론에 비해 $fp8$의 중요한 장점은 양자화의 복잡성을 피할 수 있다는 것입니다. 현재의 단점은 $fp8$ 형식을 지원하는 하드웨어 및 소프트웨어가 제한적이라는 것입니다. 사소한 단점은 NaN이 과대하게 표현되어 $(1, 5, 2)$ 및 $(1, 4, 3)$ 형식에서 각각 256개 값 중 6개(2%) 및 14개(6%)를 소비한다는 것입니다.

공개된 $fp8$ 경험적 결과는 역전파의 경우 $(1, 4, 3)$보다 $(1, 5, 2)$가 선호됨을 시사합니다. 추론(순전파)의 경우 IBM은 지수 이동이 있는 $(1, 4, 3)$을 사용하여 우수한 통계적 성능을 입증했지만, 결과는 주로 합성곱(convolutional) 모델을 대상으로 합니다. Intel은 ResNet, GNMT, Transformer, NCF 전반에 걸쳐 순전파 및 역전파 모두에 대해 $(1, 5, 2)$를 입증했습니다. 공개된 결과는 CNN 모델이 $(1, 4, 3)$의 추가 가수 비트로부터 더 많은 이점을 얻을 수 있고, 비-CNN 모델은 $(1, 5, 2)$의 추가 지수 비트로부터 더 많은 이점을 얻을 수 있음을 시사합니다. 그럼에도 불구하고 이러한 연구에 포함된 모델 수는 비교적 적으며 확실한 결론을 내리기 위해서는 추가 작업이 필요합니다.

정수 4($int4$) 지원은 최신 Nvidia GPU에서 가능합니다. 일부 CNN 모델에서의 $int4$ 추론 채택은 전력 및 메모리가 제한된 휴대폰과 같은 엣지 디바이스에서 서서히 증가할 수 있습니다. 데이터 센터에서의 채택은 매우 낮은 범위와 정밀도를 허용하는 워크로드에 대해 없거나 매우 제한적일 수 있으며, 부호 없는 $int4$로 $ReLU$ 함수의 활성화를 표현하는 것으로 제한될 수 있습니다(가중치는 $int8$로 유지). $int4$ 양자화를 개선하기 위한 연구가 진행 중입니다 [CWV+18, Don19, GMY+19].

부동 소수점 24비트($fp24$) $(1, 8, 15)$는 Alibaba 신경 처리 장치(NPU)에서 요소별 및 리덕션 연산자를 위한 CNN 모델에 사용됩니다(행렬별 연산자는 $int8 \rightarrow int16$ 사용) [JHJ+20].

Posit은 IEEE 부동 소수점 표준과는 다른 비교적 새로운 형식입니다. 이 형식은 IEEE 부동 소수점 대응물보다 적은 전력과 다이 면적을 필요로 합니다 [Gus17, Joh18]. NaN을 과대하게 표현하지 않으며 다른 장단점을 제공합니다 [dDF+19]. 그러나 이 형식은 학계에서의 채택이 미미하고 산업계에서는 채택되지 않았습니다.

로그 도메인(Log-domain)은 더 작은 수치 형식으로 통계적 성능을 유지하는 것으로 나타난 비선형 양자화의 또 다른 형태입니다 [LMC+17]. 이 형식은 학계에서의 채택이 제한적이며 산업계에서는 채택되지 않았습니다.

이진(1비트) 및 삼진(3진, -1, 0, 1을 표현하기 위한 2비트)은 연구에서, 특히 순전파 패스에서 가중치를 표현하는 데 사용되었습니다 [ROR+16, HS14].

## 다이 비용 (Die Cost)

곱셈기를 구축하는 데 드는 다이 비용과 곱셈기를 사용하는 데 드는 전력 비용은 모두 가수 비트 수에 따라 2차적으로 증가하고 지수 비트 수에 따라 선형적으로 증가합니다. 따라서 $bf16$ 곱셈기는 $fp16$ 곱셈기보다 저렴합니다. 그러나 면적 비용은 빠르게 계속 감소하고 있으므로 이 차이가 DL 하드웨어 설계 결정의 주요 요인이 되어서는 안 됩니다. 사용성 및 소프트웨어 개발 비용이 훨씬 더 중요한 요소입니다.

한 가지 형식($fp16$ 또는 $bf16$)만 지원하는 하드웨어에서의 전환을 용이하게 하기 위해, 우리는 19비트 $(1, 8, 10)$ 부동 소수점 유닛(FPU)을 사용하여 $bf16$ 및 $fp16$ 형식을 모두 지원하는 하드웨어를 설계할 것을 권장합니다. 마찬가지로 9비트 $(1, 5, 3)$ FPU를 사용하여 $(1, 5, 2)$ 및 $(1, 4, 3)$ $fp8$ 형식을 모두 지원할 것을 권장합니다. IBM에 따르면 두 형식을 모두 지원하는 데는 한 가지 형식만 지원하는 것보다 5% 더 큰 유닛만 있으면 됩니다 [SCC+19].

## 6.2 양자화 방법론 (QUANTIZATION METHODOLOGY)

$int8$을 사용하면 약간의 (1) 추가 개발과 (2) 통계적 성능 손실을 대가로 계산 성능을 향상시킬 수 있습니다. 이 섹션에서는 양자화 방법론을 설명하고 통계적 성능 손실을 완화하고 개발 과정을 줄일 수 있는 기술을 공유합니다.

$fp32$, $fp16$, 또는 $bf16$으로 훈련된 모델을 가정할 때, $int8$로 양자화하는 간단한 기술은 다음과 같습니다. 각 가중치 텐서에 대해 최대 절대값이 $\pm127$에 매핑됩니다. 활성화 텐서의 경우, 캘리브레이션 데이터셋(calibration dataset)이라고 하는 생산 데이터의 대표 샘플을 사용하여 활성화 통계를 수집하여 샘플 전체에 걸친 각 텐서의 활성화 값 분포를 찾습니다. 양자화 계수는 다음과 같습니다.

$$Q_{\mathbf{a},\mathbf{w}} = \frac{127}{\max(abs(T_{\mathbf{a},\mathbf{w}}))},$$

여기서 $T_{a, w}$는 가중치 w 또는 활성화 a에 해당하는 텐서입니다(NN에 대한 입력은 Layer 0의 활성화로 간주될 수 있음을 상기하십시오). 양자화된 값은 다음과 같습니다.

$$\begin{aligned}
\mathbf{a}_{s8} &= \Phi(Q_{\mathbf{a}}\mathbf{a}_{f32}) \in [-127, 127] \\
\mathbf{w}_{s8} &= \Phi(Q_{\mathbf{w}}\mathbf{w}_{f32}) \in [-127, 127],
\end{aligned}$$

여기서 함수 $\Phi(\cdot)$는 가장 가까운 정수로 반올림합니다.

다음 기술들은 $int8$ 추론 정확도를 향상시킬 수 있습니다. 이러한 기술을 사용하더라도 $fp32$ 정확도 대비 손실은 일부 애플리케이션에서 여전히 허용되지 않을 수 있습니다.

비대칭 양자화(Asymmetric quantization)는 스칼라와 이동(shift) 계수를 사용하여 활성화의 양자화를 개선할 수 있습니다. 가중치는 일반적으로 대략 0 평균(zero-mean)이며 대칭 양자화를 사용해야 함에 유의하십시오. 최소 활성화 값은 -128에 매핑되고 최대값은 127에 매핑됩니다.

임계값 캘리브레이션(Threshold calibration)은 배포와 유사한 데이터(라벨이 없는 데이터도 괜찮음)가 필요하며 추가적인 역전파는 필요하지 않습니다. 가장 큰 절대값을 $\pm 127$에 매핑하는 것(또는 비대칭 양자화에서 최소 및 최대 값을 각각 -128과 127에 매핑)은 아웃라이어(outlier) 숫자가 다른 숫자보다 훨씬 클 때 사용 가능한 256개의 $int8$ 값을 잘 활용하지 못하는 결과를 초래할 수 있습니다. 예를 들어, 가장 큰 숫자가 다음으로 큰 값보다 10배 더 크다고 가정해 봅시다. 그 하나의 숫자는 127에 매핑되고 나머지 값은 $[-13, 13]$에만 매핑될 수 있습니다. 아웃라이어를 무시하고 $fp32$로의 재구성 오류를 최소화하는 임계값을 찾는 것이 더 낫습니다. 일부 CNN 모델에 효과적인 또 다른 접근 방식은 더 큰 수치 표현 텐서 분포와 양자화된 텐서 분포 간의 KL-divergence(쿨백-라이블러 발산)로 측정된 정보 손실을 최소화하기 위해 아웃라이어를 잘라내는 것입니다 [Mig17]. KL-divergence는 레이어에서의 오류 메트릭을 최소화하는데, 이것이 전체 모델의 정확도 오류를 최소화하지 않을 수도 있습니다. 실제로는 값의 99% 또는 99.9%를 포착하는 임계값을 사용하기만 해도 우수한 성능 정확도를 얻을 수 있습니다.

양자화 인식 학습(Quantization aware training, QAT)은 라벨이 지정된 데이터(학습 데이터)와 역전파가 필요합니다. QAT(학습 후 양자화와 대조됨)는 양자화를 강제하면서 모델을 미세 조정하며 정확도를 향상시키는 것으로 나타났습니다. 각 학습 반복에서 양자화 대상 레이어의 가중치와 활성화는 $int8$ 값을 모방하기 위해 가짜 양자화(fake-quantized)됩니다. 역전파에 사용되는 비용은 양자화된 값을 기반으로 합니다. 기울기와 가중치 업데이트는 단정밀도로 계산됩니다. 또 다른 장점은 QAT가 양자화된 값의 재구성 오류를 최소화하므로 임계값 캘리브레이션 단계가 필요 없다는 것입니다.

선택적 양자화(Selective quantization)는 라벨이 지정된 데이터가 필요하지만 역전파는 필요하지 않습니다. softmax, tanh, sigmoid, depthwise-separable convolution, GELU, 입력 및 출력 레이어와 같은 일부 레이어는 양자화에 더 민감하므로 정확도 손실을 줄이기 위해 더 큰 수치 형식으로 유지해야 합니다 [Wu19]. softmax의 민감도는 로짓(logits)을 더 큰 수치 형식으로 누적하고 양자화하기 전에 최대 값을 빼서 약간 줄일 수 있습니다 [BHH20]. GELU의 활성화 출력은 예를 들어 10으로 잘라내어(clip) 일부 $int8$ 값이 GELU 음수 활성화 값을 나타내도록 할 수 있습니다.

헤세 행렬(Hessian matrix)의 트레이스(trace) 근사값을 분석하여 레이어의 민감도를 평가하는 것이 좋습니다. 이 기술을 사용하면 최소한의 정확도 손실로 일부 레이어의 수치 형식을 4비트로 줄일 수 있습니다 [DYC+19]. 민감도를 평가하는 덜 신뢰할 수 있지만 계산이 더 빠른 다른 메트릭으로는 KL-divergence와 재구성된 $fp32$ 모델과의 평균 제곱근 오차(RMSE)가 있습니다. 강화학습(RL)은 특정 하드웨어 타겟에 대해 지연 시간, 에너지 및 정확도에 최적화된 양자화 모델 설계를 용이하게 할 수 있습니다. 선택적 양자화를 위한 가능한 알고리즘은 다음과 같습니다.

#### 알고리즘 6.2: 양자화 기술


```

모든 레이어를 양자화하고 각 레이어에 대한 평균 헤세 트레이스를 근사화 [DYC+19]
최대 허용 정확도 오류 E 설정

while 정확도 오류 > E do
가장 높은 평균 헤세 트레이스를 가진 8비트 (또는 4비트) 레이어를 비양자화(Unquantize)

```

이 알고리즘은 양자화할 수 있는 레이어를 결정합니다. 한 가지 과제는 큰 수치 형식과 작은 수치 형식의 레이어를 인터리빙(interleaving)하면 많은 변환 오버헤드로 인해 계산 비용이 높아질 수 있다는 점입니다.

계층 간 범위 균등화(Cross-layer range equalization)는 데이터가 필요 없는 양자화(데이터 및 역전파 필요 없음)입니다. 레이어 간 가중치의 범위를 균등화하고, 레이어 사이에 구간별 선형 활성화 함수($ReLU$와 같은)가 사용된다는 가정 하에 활성화 범위를 제약합니다 [NvB+19]. 이 제약 조건은 많은 CNN 모델에서는 만족되지만 비-CNN 모델에서는 만족되지 않습니다. 이 기술은 Qualcomm Neural Processing SDK에서 사용됩니다.

채널별 양자화(Channel-wise quantization)는 전체 텐서에 대해 하나의 계수를 사용하는 대신 각 채널에 대해 양자화 계수를 사용합니다.

확률적 반올림(Stochastic rounding)은 양자화 계수를 곱한 후 가장 가까운 값 반올림 대신 확률적으로 반올림하여 성능을 향상시킬 수 있습니다 [WCB+18]. 예를 들어, 숫자 1.2를 숫자 1로 반올림하는 대신 80% 확률로 1로, 20% 확률로 2로 반올림합니다.

부호 없는 $int8$ $ReLU$ 활성화는 $ReLU$ 함수의 활성화에 대해 부호 있는 $int8$ 대신 부호 없는 $int8$ 표현을 사용합니다. 부호 있는 $int8$을 사용하면 모든 활성화가 음수가 아니므로 값의 절반이 낭비됩니다.

QAT, 선택적 양자화, 채널별 양자화, 확률적 반올림 기술은 $fp8$에도 도움이 됩니다 [CBG+20].

## 6.3 가지치기 및 압축 (PRUNING AND COMPRESSION)

학습된 모델에는 일반적으로 0에 가까운 여러 가중치가 있습니다. 이를 가지치기(pruning)하는 것, 즉 어떤 작은 $\epsilon$ 값보다 작은 모든 가중치를 0으로 강제하면 희소 모델이 생성됩니다. $\epsilon$에 대한 좋은 값을 선택하려면 실험이 필요합니다. 가지치기는 모델의 크기를 줄이기 위해 수십 년 동안 사용되어 왔습니다. 흥미로운(하지만 우연의 일치일 가능성이 높은) 부가적인 사실은 생물학적 뉴런을 가지치기하는 것이 건강한 발달에 중요하다는 점입니다 [Iva71, LDS89, JS18, Wal13]. 가지치기는 희소 연산자를 사용하여 연산 수를 줄일 수 있지만, 가지치기의 주요 이점은 압축을 통해 메모리 사용량을 줄이고 메모리 대역폭 제약을 완화하는 것입니다. 10.1절에서 논의된 AutoML을 사용하여 컴팩트한 토폴로지를 학습할 수 있음에 유의하십시오 [HLL+19].

약간의 가지치기를 수행하는 것은 가지치기 양에 따라 다르지만 일반적으로 통계적 성능에 최소한의 영향을 미칩니다. 어떤 경우에는 가지치기가 정규화의 한 형태이므로 성능을 향상시킬 수도 있습니다. 통계적 성능에 영향을 주지 않고 모델을 가지치기할 수 있다는 것은 모델이 과매개변수화되었음을 의미합니다. 가설 중 하나는 솔루션 공간을 더 잘 탐색하고 더 평평한 최소값을 찾기 위해 과매개변수화된 모델이 필요하다는 것입니다. 모델을 학습한 후에는 그러한 많은 매개변수가 더 이상 필요하지 않습니다. 관련 가설은 복권 가설(Lottery Ticket)입니다. 큰 모델 안에는 더 큰 모델과 동일하거나 더 나은 성능을 가진 더 작은 모델(복권 당첨자)이 존재한다는 것입니다 [FC19].

모델 희소성에는 구조적(structured) 및 비구조적(unstructured) 두 가지 유형이 있습니다. 구조적 희소성 학습(SSL)은 전체 벡터, 배열 또는 텐서를 가지치기합니다. SSL은 전체 매개변수 및 계산 수를 줄입니다. 예를 들어 합성곱 필터를 제거하는 식입니다 [MHP+17]. 다양한 SSL 기술이 개발되었습니다 [WWW+16, HGD+17, ZTZ+18, ZDH19, HZS+19, LSZ+19]. CPU 및 GPU에서 구조적 희소성(비구조적 희소성과 달리)은 연산 수를 줄일 수 있습니다.

![Image](/assets/img/posts/2026-02-10-DeepLearningSystems_chapter6_artifacts/figure6.2.png)

그림 6.2: 0에 가까운 가중치(링크)를 제거하여 모델 가지치기.

비구조적 희소성은 그림 6.2와 같이 텐서의 전체 구조에 영향을 주지 않으면서 텐서 전체의 값을 가지치기합니다. 비구조적 희소 가지치기된 모델은 희소도가 90% 이상일 때 Nvidia cuSPARSE 및 Intel oneMKL 라이브러리의 BLAS 기능을 활용할 수 있습니다. 그러나 대부분의 희소 모델은 이러한 라이브러리의 희소 GEMM 기능으로부터 큰 이점을 얻을 만큼 충분한 희소성을 갖지 못합니다. 대안으로 Google, DeepMind, Stanford는 적당히 희소한 Transformer 및 MobileNet 모델에서 정확도를 희생하지 않고 Nvidia V100 GPU에서 $1.2\times-2.1\times$ 속도 향상과 최대 $12.8\times$ 메모리 절감을 달성하는 기술을 개발했습니다 [GZY+20].

대부분의 생산 하드웨어는 밀집(dense) 행렬 연산을 위해 설계되었습니다. 희소 피연산자를 지원하는 하드웨어는 제한적이며, 한 예로 LNPU 장치가 있습니다 [LLH+19]. Nvidia A100 GPU는 $2\times$ 더 많은 연산으로 세분화된(fine-grained) 구조 희소성을 지원합니다.

가지치기를 위한 기술은 다음과 같습니다.

- 더 많은 가중치를 0 근처로 강제하기 위해 더 큰 가중치 감쇠(weight decay)로 학습;
- 가지치기된 모델 미세 조정(라벨이 지정된 데이터 및 역전파 필요) [HPN+17];
- 학습 과정 전반에 걸쳐 가지치기: 각 학습 반복에서 작은 가중치를 0으로 설정 [LCZ+19].

전력이 제한된 엣지 디바이스의 경우 에너지 인식 가지치기가 필요할 수 있습니다. 즉, 에너지를 가장 많이 소비하는 레이어를 가지치기하는 것입니다 [YCS17].

가지치기된 모델은 적대적 공격(adversarial attacks)에 덜 강건합니다. 적대적 공격은 인간이 변화를 감지하지 못하도록 NN에 대한 입력을 교묘하게 변경하지만 모델은 매우 다른 출력을 생성할 때 발생합니다. 예를 들어, 모델은 지각할 수 없게 변경된 버스 이미지를 타조라고 높은 신뢰도로 예측합니다. 적대적 훈련 모델 압축(ATMC) 및 방어적 양자화(Defensive Quantization)는 가지치기와 이러한 공격에 대한 강건성 보장 사이의 균형을 제공하는 기술입니다 [GWY+19, LGH19].

모델 압축은 압축 해제를 위한 약간의 추가 연산을 대가로 메모리 및 대역폭 요구 사항을 줄입니다. 이러한 추가 연산 시간은 종종 감소된 대역폭 제약에서 절약된 시간에 비해 작습니다. 따라서 압축은 일반적으로 유리합니다. 압축되지 않은 비구조적 희소 모델과 밀집 모델은 동일한 메모리 사용량을 갖습니다. 압축되지 않은 0 값을 저장하는 데는 다른 값과 동일한 비트 수가 필요하기 때문입니다. 허프만 코딩과 같은 압축 알고리즘은 0 값과 같은 일반적인 값을 인코딩하기 위해 1비트를 사용합니다. 또 다른 기술은 유사한 값을 클러스터링하고 적은 비트로 양자화하여 각 그룹이 양자화 계수를 갖도록 하는 것입니다 [HKK16].

ReLU 함수가 있는 모델은 희소 활성화를 가지며, 그 희소성은 모델 깊숙이 들어갈수록 활성화에 대해 커집니다. 순전파 학습 단계 동안, 희소 활성화를 저장(역전파 단계에 사용하기 위해)하기 전에 압축하면 대역폭 병목 현상을 완화할 수 있습니다.

## 6.4 지식 증류 (KNOWLEDGE DISTILLATION)

지식 증류(KD)는 Bucila 등의 연구를 기반으로 하는 모델 압축 기술이며 빠르게 채택되고 있습니다 [HVD15, BCN06]. KD는 특정 작업에 대한 메모리 및 계산 요구 사항을 줄이며 압축 해제 단계가 필요하지 않습니다. KD는 전이 학습(transfer learning)과 관련이 있습니다. 복잡한 모델(교사 모델)의 지식이 더 간단한 모델(학생 모델)로 증류됩니다. 학생 모델은 교사 모델에 사용된 것보다 더 작은 데이터셋과 더 큰 학습률(LR)을 사용하여 학습됩니다.

![Image](/assets/img/posts/2026-02-10-DeepLearningSystems_chapter6_artifacts/figure6.3.png)

그림 6.3: 지식 증류. 큰 교사 모델이 지식을 더 작은 학생 모델로 증류합니다. 학생 모델은 일반적인 softmax와 교사 모델의 softed softmax를 모두 사용하여 학습합니다. [Int18]에 기초함.

학습된 교사 모델은 학생의 학습 데이터셋에 대해 완화된(softened) 확률 출력을 생성합니다. 학생 모델은 그림 6.3과 같이 교사의 완화된 확률 출력과 유사한 출력을 생성하도록 학습됩니다. softmax 온도(temperature)라고도 하는 완화된 softmax는 먼저 로짓을 정규화하기 전에 어떤 값 T > 1 (온도라고 함)로 나눕니다. 출력은 클래스 유사성을 더 잘 포착하는 완화된 확률 분포입니다. 예를 들어, 숫자 7이 있는 입력 이미지에 대한 숫자 분류에서의 완화된 출력은 7에 대해 가장 높은 값을 가져야 하며, 1이나 9와 같이 7처럼 보이는 숫자에 대해서도 비교적 높은 값을 가져야 합니다. 학생 모델은 (1) softmax 온도를 사용하는 완화된 출력과 (2) 일반 softmax를 사용하는 원-핫(one-hot) 정답 벡터를 학습하도록 훈련됩니다. softmax 온도는 모델에 정규화도 제공합니다 [YTL+19].

KD의 직관은 교사 모델이 다양한 클래스 간의 관계를 학습하기 위해 더 복잡한 모델을 필요로 한다는 것입니다. 정답 원-핫 벡터는 클래스 유사성을 인코딩하지 않으며 각 클래스를 완전히 독립적인 것으로 취급합니다. 교사 모델은 학생 모델에 클래스 관계를 제공합니다. 따라서 학생 모델은 처음부터 이를 학습할 필요가 없으며 더 간단한 토폴로지를 사용할 수 있습니다.

이 작업의 확장으로는 학생 앙상블이 softmax 출력을 공유하여 협력적으로 학습하고 서로 가르치는 심층 상호 학습(deep mutual learning, DML)과, 대형 교사 모델의 지식을 중간 크기의 TA 모델로 증류한 다음 더 작은 크기의 학생 모델로 증류하는 교사 보조(teacher assistant, TA)가 있습니다 [ZXH+17, MFL+19].

이 장에서는 생산 환경에서 사용되는 다양한 수치 형식과 연구자들이 탐구 중인 형식, 그리고 모델의 메모리 사용량을 줄이기 위한 압축 기술에 대해 자세히 설명했습니다. 더 작은 수치 표현을 사용하면 사이클당 연산 횟수를 늘리고 메모리, 메모리 대역폭, 네트워크 대역폭 및 전력 소비를 줄일 수 있습니다. 그러나 특히 일부 $int8$ 모델의 경우 통계적 성능이 저하될 수 있습니다. 우리는 이러한 정확도 손실을 완화하기 위한 양자화 기술의 발전을 논의했으며 헤세 기반 분석이 양자화 가능한 레이어를 결정하는 유망한 경로임을 확인했습니다. 수치 형식 전반에 걸친 하드웨어 지원은 중요한 하드웨어 설계 결정 중 하나입니다. 우리는 학습 프로세서는 하나만 지원하는 것보다 작은 다이 비용을 고려하여 $bf16$과 $fp16$을 모두 지원하고 일부 $fp32$를 지원할 것을 권장하며, 추론 프로세서는 주로 학습 형식과의 호환성을 위해 $fp16$, $bf16$을 지원하고, $int8$ 및 $fp8$과 일부 $fp32$를 지원할 것을 권장합니다. 다음 장에서는 컴퓨터 아키텍처의 기본 사항을 검토하고 다양한 DL 하드웨어 설계에 대해 논의합니다.
